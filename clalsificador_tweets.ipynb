{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se limpi√≥ el dataset de tweets ironicos y se agreg√≥ etiqueta\n"
     ]
    }
   ],
   "source": [
    "#es necesario limpiar el archivo de tweets para que no se sesgue la red neuronal con las\n",
    "#etiquetas de iron√≠a y sarcasmo\n",
    "infile = \"ironicos_reduced.txt\"\n",
    "#agregaremos los tweets etiquetados a un archivo\n",
    "outfile = \"tweets_limpios.txt\"\n",
    "\n",
    "delete_list = [\"#iron√≠a\", \"#ironia\", \"#sarcasmo\", \"sarcasmo\",\",\"]\n",
    "fin = open(infile)\n",
    "fout = open(outfile, \"w+\")\n",
    "for line in fin:\n",
    "    for word in delete_list:        \n",
    "        line = line.lower().replace(word, \"\")          \n",
    "        line = line.replace(\"\\n\", \", 1\\n\")  \n",
    "        line = line.replace(\"1 1 1 1\", \"\")\n",
    "    fout.write(line)\n",
    "fin.close()\n",
    "print \"Se limpi√≥ el dataset de tweets ironicos y se agreg√≥ etiqueta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se limpi√≥ el dataset de tweets no ironicos y se agreg√≥ etiqueta\n"
     ]
    }
   ],
   "source": [
    "#agregamos al dataset el conjunto de tweets no ir√≥nicos etiquetados\n",
    "infile = \"no_ironicos_reduced.txt\"\n",
    "\n",
    "fin = open(infile)\n",
    "for line in fin:\n",
    "    for word in delete_list:        \n",
    "        line = line.lower().replace(\",\", \"\")          \n",
    "        line = line.replace(\"\\n\", \", 0\\n\")   \n",
    "        line = line.replace(\"0 0 0 0\", \"\")\n",
    "    fout.write(line)\n",
    "fin.close()\n",
    "fout.close()\n",
    "print \"Se limpi√≥ el dataset de tweets no ironicos y se agreg√≥ etiqueta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convertimos a csv\n",
    "with open('tweets_limpios.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split(\",\") for line in stripped if line)\n",
    "    with open('tweets.csv', 'w') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow(('tweet', '1'))\n",
    "        writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of                                                   tweet  1\n",
       "0                                                 tweet  1\n",
       "1                                 se nota el inter√©s.    1\n",
       "2            este a√±o he aprendido mucho de ingl√©s...    1\n",
       "3     uy trump! luego de la fuga del #chapo estoy to...  1\n",
       "4     que fresquito mas bueno hace en la calle no?  ...  1\n",
       "5     hace calor abro la ventana y no hay anda mejor...  1\n",
       "6     gracias por dejarme sin internet @tigo_costari...  1\n",
       "7          en el anuario de este a√±o se han lucido...    1\n",
       "8                           que bien empezamos el dia    1\n",
       "9     para nada hay corrupci√≥n y negocios il√≠citos e...  1\n",
       "10    es incre√≠ble lo rapid√≠simo que me van los dato...  1\n",
       "11    te conf√≠o una vida pero no el dinero üòï üòì üò®ÔøΩ...  1\n",
       "12               espa√±a pa√≠s organizado y trabajador.    1\n",
       "13    @enclmdiario como va a ser cierto esto si @mdc...  1\n",
       "14    creo que en el grupo de nosotros no podemos se...  1\n",
       "15    cuando estas en esos d√≠as y te resfrias adem√°s...  1\n",
       "16             ahora a esperar la continuaci√≥n. yuju.    1\n",
       "17                   ojal√° tuiteara tan chido como t√∫.   1\n",
       "18    ¬°¬°¬°no hay nada mejor q un dolor de cabeza y es...  1\n",
       "19                   tengo tantas ganas de comer sopa    1\n",
       "20    gracias a todas mis fans por estar tan pendien...  1\n",
       "21                mi internet funciona de lujo oye...    1\n",
       "22           llegar a esta casa es toda una alegria..    1\n",
       "23                    mi vida en el amor que bella es    1\n",
       "24    ya echaba de menos este olor caracteristico de...  1\n",
       "25                      me encanta no poder dormir xd    1\n",
       "26    igual canta alejandro sanz que su equipo igual...  1\n",
       "27    es re divertido salir a la calle y tener miedo...  1\n",
       "28    hormonas femeninas: peque√±as cabroncillas que ...  1\n",
       "29                       mira t√∫ que mont√≥n de planes    1\n",
       "...                                                 ... ..\n",
       "5971  la vida es perfecta cuando empiezas a ver los ...  0\n",
       "5972  la vida es como una corriente del mar ya que s...  0\n",
       "5973                 √öltimamente no twitteo como antes   0\n",
       "5974  450 personas participaron este domingo en jorn...  0\n",
       "5975  el d√≠a que tenga a alguien a mi lado primero l...  0\n",
       "5976  cuando me habla / cuando me friendzonea http:/...  0\n",
       "5977  @ pero ni un toque eh voy a estudiar ma√±ana en...  0\n",
       "5978                                         @ como tu   0\n",
       "5979                            cuando me i va o antes   0\n",
       "5980  @ @ @ aaaaay que cosa tan fea y como dice vane...  0\n",
       "5981  @ que sea este lunes un buen inicio de semana ...  0\n",
       "5982  yo me re muero cuando anita me dice que esta m...  0\n",
       "5983  @ @ primero deja de mamar con que tienes mucha...  0\n",
       "5984  @ te quiero much√≠simo mas! no dejes que desapa...  0\n",
       "5985  se est√° acabando la intensidad necesito el com...  0\n",
       "5986  el que perdona tiene tanto que perder cuantos ...  0\n",
       "5987  as√≠ r√°pido como lleg√≥ el lunes lleg√≥ junio. co...  0\n",
       "5988  atletico de madrid utilizaria a raul jimenez c...  0\n",
       "5989  @ capi m√°s cuidado con la ortograf√≠a de los te...  0\n",
       "5990  uzb 1 - 2 hon (ht) - @ siempre ha terminado pe...  0\n",
       "5991  el tulpiel esta muy de moda...\\nlo encuentras ...  0\n",
       "5992  luca en cuero como si fuera verano jajajajajaj...  0\n",
       "5993  @ quisieron al pap√∫ gomez que tiene tatuado el...  0\n",
       "5994                como la de la peli mielda jajajaja   0\n",
       "5995  como  como  como y no engordo  que mas puedo p...  0\n",
       "5996  cuando deje los voy a extra√±ar banda a casi to...  0\n",
       "5997  cuando se espera y se vive con fidelidad grand...  0\n",
       "5998  my teenchoice nominee for choicemusicgroupmale...  0\n",
       "5999                      tambi√©n me pasa. http://link   0\n",
       "6000  una verguenza quedar campe√≥n asi pero santos n...  0\n",
       "\n",
       "[6001 rows x 2 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checamos el encabezado de los datos, 1 representa que es ironico, 0 que no lo es\n",
    "tweets = pd.read_csv('tweets.csv', names = [\"tweet\", \"1\"])\n",
    "tweets.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6001.0</td>\n",
       "      <td>0.500083</td>\n",
       "      <td>0.500042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count      mean       std  min  25%  50%  75%  max\n",
       "1  6001.0  0.500083  0.500042  0.0  0.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#descripci√≥n de los datos\n",
    "tweets.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6001, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = tweets.drop(\"tweet\",axis=1)\n",
    "X = tweets[\"tweet\"]\n",
    "\n",
    "#separamos los datos para entrenamiento y experimento\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tama√±o del dataset de entrenamiento\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1501,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tama√±o del dataset de prueba\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "fsw = file(\"spanish_stop_words.txt\")\n",
    "stop_w = fsw.read().splitlines()\n",
    "vectorizer = TfidfVectorizer(stop_words = fsw)\n",
    "X_train = vectorizer.fit_transform(X_train).todense()\n",
    "X_test = vectorizer.fit_transform(X_test).todense()\n",
    "\n",
    "#Fit de los datos de entrenamiento\n",
    "#Primero escalamos los datos\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "StandardScaler(copy=False , with_mean=False, with_std=False)\n",
    "\n",
    "#Aplicamos la escala a la transformaci√≥n de los datos:\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karlagarcia/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 5, 5), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=150, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generamos el modelo de perceptron multicapas \n",
    "#pasamos como n√∫mero m√°ximo de iteraciones y 5 capas en cada neurona\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5,5,5),max_iter=150)\n",
    "mlp.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2215    0]\n",
      " [  11 2274]]\n"
     ]
    }
   ],
   "source": [
    "#realizamos las predicciones sobre los tweets de prueba\n",
    "predictions = mlp.predict(X_train)\n",
    "print(confusion_matrix(y_train,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = \"reporte.txt\"\n",
    "fout = open(outfile, \"w+\")\n",
    "fout.write(classification_report(y_train, predictions))\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlp.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlp.coefs_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlp.intercepts_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
